---
title: "Week 2 Notes"
author: "ishaka"
date: "12 November 2015"
output: html_document
---

# Important concepts
Some important concepts to remember:  
- Difference between **sample** and **population**. Due to real world limitations we will never be able to measure the whole population, we can only measure a sample of the population.  
- This means that we will never be able to measure the population mean and population variance. The population mean and population variance are just **theoretical values**  

```{r}
set.seed(1234)
```

# Sample mean
The mean of the sample will be the same as the population(theoretical) mean. 

Let's run an example: 
Assume we are a scientist researching the lifetime of bacteria and say the bacteria's lifetime follows a standard uniform distribution. Let's take a sample of 1 bacteria using R

```{r}
sample <- runif(1)
sample
```

And plot the lifetime 
```{r}
plot(sample)
```

Well that wasn't very helpful isn't? That's because we only have one bacteria, let's increase our sample size to 10 bacteria!
```{r}
#generate 10 numbers based on standard uniform distribution
sample <- runif(10)
plot(sample)
```

We can see that the lifetimes of the 10 bacteria's are uniformly distributed. Let's find out where the mean is
```{r}
mu <- mean(sample)
mu
```

The mean is `r mu`

Let's get the frequency and look where the mean is
```{r}
hist(sample)
abline(v = mean(sample), lwd=2, col="red")
```


What if we have 1 million bacteria in our sample?
```{r}
sample <- runif(1000000)
hist (sample)
abline(v = mean(sample), lwd=2, col="red")
```

They are more evenly distributed, but what about the mean?
```{r}
mu <- mean(sample)
mu
```

The mean of 1 million bacteria is `r mu` which is closer to the *theoretical(population) mean* of standard uniform distribution of 0.5. 

This means that the sample mean becomes closer to the theoretical mean as we increase the number of samples, in other words the *standard error of the mean* becomes smaller as we increase the number of samples, which brings us to the next section.

# Standard error of the mean (aka standard error aka SE)
Since "standard error of the mean" is quite a mouthful statisticians normally calls it "standard error" and since that is still a mouthful, they have shortened it to just "SE"

Standard error is basically an indicator of how accurate your sample mean is in relation to the true(population) mean.

So how do we measure SE? Thankfully some really smart people has worked out a formula  

$$SE_{mean} = \sigma/\sqrt{n}$$

Where $\sigma$ is the *population(theoretical)* standard deviation of the distribution.

The formula actually makes sense because:  
- as you increase the sample size, the error decreases as we see in the bacteria example above. (This is related with the "Law of Large Numbers" theorem.)  
- if your distribution does not have any variance/standard deviation (i.e. it always returns the expected value). then your SE is 0. Similarly if your variance/standard deviation is a huuuge number, you will need a looooooot of samples to reduce the standard error of your sample mean

# Variance
Variance is basically how spread out your sample will be from the expected value. Standard deviation is the square root of variance. We should work with standard deviation because it is in the same unit as the mean.

The formula for calculating the theoretical(population) variance is  
$Var(X) = E[(X-\mu)^2]$  
  
or  
  
$Var(X) = E[X^2] - E[X]^2$  

Sounds good so far but we are working with the sample, not the population, is there a way to calculate the sample variance? 

Yes! The smart people have worked out a formula for the sample variance too:

$$S^2 = \frac{\sum_{i=1} (X_i - \bar X)^2}{n-1}$$

You might ask why is it $n-1$ at the bottom part? Well that is to make it "unbiased". I have no idea why, some things are easier if you just believe it.

That's a really complicated formula, so thankfully we can use R to calculate the variance of our sample. Let's use our bacteria experiment for examples again

What is the variance if we have 10 bacterias?
```{r}
sample <- runif(10)
var <- var(sample)
var
```

The variance of 10 bacteria is `r var`

What if we have 10 million bacterias in our sample?
```{r}
sample <- runif(10000000)
var <- var(sample)
var
```

The variance of 10 million bacteria is `r var`

Since the bacteria follows standard uniform distribution, the theoretical(population) variance is 1/12 (`r 1/12`). We can see that as the number of sample increases the sample variance approaches the population(theoretical) variance.

# Distributions of sample mean
Going back to our bacteria experiments, it would be a folly to just do the experiment once. We would like to repeat it several times to make sure it is not a fluke.

Repeating the experiment multiple times will give us multiple means and multiple variance. One mean and variance result for each experiment.

Let's find out more about the distribution of the sample means:  
- what is the expected value (mean) of this distribution (of sample means)  
- what is the variance of this distribution (of sample means)

## Expected value for distribution of sample mean
Turns out that the expected value of the distribution is the same as the sample mean, which is also the same as the population mean.
$E[\bar X]=\mu$

This is great news because we can take the average of average readings and it will be the same as the theoretical average!

Let's try to run this in R using our bacteria experiments. Assume due to limited space in the lab we can only measure 5 bacteria at a time, but we can run the experiments multiple times. Let's try repeating the experiment 5 times first.
```{r}
seven_means = NULL
for (i in 1:7) seven_means = c(seven_means, mean(runif(5)))
hist(seven_means)
```

What is the expected value of the means?
```{r}
mean(seven_means)
```

Which is relatively close to the population average.

What if we repeat the experiments 10000 times?
```{r}
ten_thousand_means = NULL
for (i in 1:10000) ten_thousand_means = c(ten_thousand_means, mean(runif(5)))
hist(ten_thousand_means)
```

What is the expected value of the means now?
```{r}
mean(ten_thousand_means)
```

We can see the the expected value of the distribution of mean is the same as the sample mean, which is also the same as the population mean.

## Variance of distribution of sample mean
The variance for the distribution of sample mean can be expressed as 
$$Var(\bar X) = \sigma^2/n$$  

where:  
- $\sigma^2$ is the variance of the distribution  
- $n$ is the number of samples in a single experiment

So as the number of samples (in the distribution) increases, the variance decreases. This makes sense again because the more samples we have, the more accurate the distribution will be.

Using the same sets of means we used before (uniform distribution with $n =$ 5) , we can try calculating the variance of sample mean of standard uniform distributions

The $\sigma^2$ of standard uniform distribution is 1/12 so plugging into the formula the $Var (\bar X)$ is 
```{r}
1/12/5
```

Let's find out the variance of the distribution of the mean for five experiments
```{r}
var(seven_means)
```


How about the variance of distribution for the ten thousand experiments?
```{r}
var(ten_thousand_means)
```

Here we can see **the variance of the distribution of the sample means does not change significantly how often we repeat the experiments if the number of samples is low**.

So let's see what happens if we increase the number of sample in our experiment from 5 bacteria to 100 bacteria.

According to the formula the variance of the distribution of sample means should be 
```{r}
1/12/100
```

Let's see if this is the case for when we repeat the experiments seven times?
```{r}
seven_means_b = NULL
for (i in 1:7) seven_means_b = c(seven_means_b, mean(runif(100)))
var(seven_means_b)
```

What about the variance of sample means when we repeat the experiment 10,000 times?
```{r}
ten_thousand_means_b = NULL
for (i in 1:10000) ten_thousand_means_b = c(ten_thousand_means_b, mean(runif(100)))
var(ten_thousand_means_b)
```

We can see that the variance of the distribution of sample mean becomes smaller when the sample size increases.

# Central Limit Theorem (CLT)
You might notice that the distribution of means follows a normal distribution as we increase the number of experiments, even though they are the averages from a uniform distribution. Why is this the case?

Turns out this is how the world works! Just like how some things in the world follows normal distribution, the distribution of averages (of iid variables) will always follow a normal distribution, irrespective of whatever distribution the averages are from.

CLT states that the distribution of averages of iid variables (properly normalized) becomes that of a standard normal as the sample size increases

In other words:
- The distribution of *means* from a uniform distribution will be a normal distribution.  
- The distribution of *means* from an exponential distribution will be a normal distribution.  

The central limit theorem will only works in certain conditions though:  
- The underlying distribution must have a well-defined expected value and a well-defined variance  
- The underlying distribution must have independent and identically distributed (iid) random variables  

# Confidence Interval
In a normal distribution the probability of event occuring mean + 1.96 SD is approx. 2.5%. Similarly the probablity of event occuring mean - 1.96 is approx 2.5* also.

Combining them, the probability of an event occuring outside of 1.96 SD from the mean is approximately 5%. In other word there is a 95% probability that the event will occur within 1.96 ( or around 2) SD.

Remember that for the distribution of mean ($\bar X$)  
- mean  : $\mu$  
- SD    : $\sigma/\sqrt{n}$

So the probability of the $\bar X$ occuring within the middle 95% band is $$\mu \pm \frac{2\sigma}{\sqrt{n}}$$

For example:
```{r}
library(UsingR) # you may need to install the package to your R
data(father.son)

# lets find the confidence interval for the average son height (in inches)
x <- father.son$sheight
n <- length(x)
CI <- mean(x) + c(-1,1) * qnorm(0.975)* sd(x)/sqrt(n)

# divide by 12 to convert inches to feet
CI/12
```

### Wald confidence interval
For Bernoulli distributions, $X_i$ is 0 or 1 with success probability $p$ and the variance is $\sigma^2 = p(1 - p)$
* the confidence interval takes the form of $$\hat{p} \pm z_{1-\alpha/2}\sqrt{\frac{p(1-p)}{n}}$$

* since the population proportion $p$ is unknown, we can use the sampled proportion of success $\hat{p} = X/n$ as estimate

* $p(1-p)$ is largest when $p = 1/2$, so 95% confidence interval for $p$ can be estimated by  

$$\begin{aligned}
\hat{p} \pm Z_{0.95} \sqrt{\frac{0.5(1-0.5)}{n}} & = \hat{p} \pm qnorm(.975) \sqrt{\frac{1}{4n}}\\
& = \hat{p} \pm 1.96 \sqrt{\frac{1}{4n}}\\
& = \hat{p} \pm \frac{1.96}{2} \sqrt{\frac{1}{n}}\\
& \approx \hat{p} \pm \frac{1}{\sqrt{n}}\\
\end{aligned}$$

For example:
You are running for a political office. From a random sample of 100 voters, 56 are likely to vote for you. What is your confidence interval?

```{r}
#sample 
n <- 100

# p-hat (56 divide by 100)
phat <- .56

# 95% estimate is
phat + c(-1,1) * 1/(sqrt(n))
```

Note that the lower estimate is less than 0.5, which means we cannot rule out possibilities below 0.5 with 95% confidence. So you are not save yet!

Rough guidliness, you will need 100 samples for 1 decimal place accuracy, 10,000 for 2, and 1,000,000 for 3

```{r}
# You can also use another method to calculate the confidence interval for binomial distribution
binom.test(56,100)$conf.int
```

### Poison interval
How do we calculate CI for Poisson distribution?

Say a nuclear pump fails 5 times out of 94.32 days, give a 95% CI for the failure rate per day.

```{r}
x <- 5
t <- 94.32
lambda <- x/t

# calculate CI. Remember that for Poison interval: mean = lambda. and sample variance is lambda/t
CI <- lambda + c(-1,1) * qnorm(0.975) * sqrt(lambda/t)

# round the result to 3 decimals place
round(CI, 3)
```

We can also using the built in R function to calculate the poison interval CI!
```{r}
poisson.test(5, T = 94.32)$conf
```

You can see the the built in R function gives you a more convervative "coverage". It is wider so it will guarantee you a 95% coverage. The confidence interval becomes wider as the coverage increases, because you want to be sure the interval will contain the parameter.



